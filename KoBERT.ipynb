{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suitable-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pharmaceutical-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "classified-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ed062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open('./data/val.pickle', 'rb') as f:\n",
    "    val = pickle.load(f)\n",
    "\n",
    "with open('./data/test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convinced-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "middle-attention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cc1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = [[text[0][0] + ' ' + text[0][1] + ' ' + text[0][2], str(text[1]) ] for text in train]\n",
    "dataset_validation = [[text[0][0] + ' ' + text[0][1] + ' ' + text[0][2], str(text[1]) ]  for text in val]\n",
    "dataset_test = [[text[0][0] + ' ' + text[0][1] + ' ' + text[0][2], str(text[1]) ]  for text in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30a2b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['사람이 되고 싶어서 그대가 말한 온갖 작품을 가슴 속에 새기고', '1'],\n",
       " ['내 모습에 한숨 쉬네 오랜만에 느껴지는 이 떨림이 날 단순하게 만들어', '1'],\n",
       " ['참 오래됐지 우리 서로 헤어진 지 나도 네가 없는 삶에 많이 익숙해졌어', '0'],\n",
       " ['욕심을 잃고 초심에 대한 촛농을 녹였지 얼마나 뜨거울지 몰라', '1'],\n",
       " ['아예 선을 그어 주던가 네가 나를 잡던가 잡힐 손을 주던가 오늘도 이렇게 너를 보낸다', '1'],\n",
       " ['이제 지난 일로 간직할게 매일 찾아오는 공허함을 버틴다는 게 어렵지만', '0']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da262497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "devoted-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7c5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = nlp.data.BERTSentenceTransform(tokenizer=tok, max_seq_length=max_len, pad=True, pair=False)  # 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a486a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "    \n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expanded-findings",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_validataion = BERTDataset(dataset_validation, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "serial-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "validation_dataloader = torch.utils.data.DataLoader(data_validataion, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "laden-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "funded-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "enabling-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personal-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sufficient-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "searching-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cheap-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return acc\n",
    "\n",
    "def calc_precision(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    right = (max_indices == Y).sum().data.cpu().numpy()\n",
    "    pre = right/max_indices.size()[0]\n",
    "    return pre\n",
    "\n",
    "def calc_recall(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    right = (max_indices == Y).sum().data.cpu().numpy()\n",
    "    re = right/max_indices.size()[0]\n",
    "    return re\n",
    "\n",
    "def calc_f1(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    right = (max_indices == Y).sum().data.cpu().numpy()\n",
    "    pre = right/max_indices.size()[0]\n",
    "    \n",
    "    right = (max_indices == Y).sum().data.cpu().numpy()\n",
    "    re = right/max_indices.size()[0]\n",
    "    \n",
    "    f1 = 2*pre*re/(pre+re)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "worth-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1\n",
      "Train : loss 0.732 Acc 0.516\n",
      "Val : Acc 0.715 \n",
      "epoch 2 batch id 1\n",
      "Train : loss 0.365 Acc 0.859\n",
      "Val : Acc 0.749 \n",
      "epoch 3 batch id 1\n",
      "Train : loss 0.216 Acc 0.922\n",
      "Val : Acc 0.739 \n",
      "epoch 4 batch id 1\n",
      "Train : loss 0.040 Acc 0.984\n",
      "Val : Acc 0.743 \n",
      "epoch 5 batch id 1\n",
      "Train : loss 0.017 Acc 0.984\n",
      "Val : Acc 0.754 \n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        \n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        \n",
    "        if batch_id % log_interval == 0:\n",
    "            print('epoch {} batch id {}'.format(e+1, batch_id+1))\n",
    "            print(\"Train : loss {:.3f} Acc {:.3f}\".format(loss.data.cpu().numpy(), train_acc / (batch_id+1)) )\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(validation_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        val_acc += calc_accuracy(out, label)\n",
    "        \n",
    "    print(\"Val : Acc {:.3f} \" .format(val_acc / (batch_id+1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd4a598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pred = []\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "model.eval()\n",
    "# for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "#     token_ids = token_ids.long().to(device)\n",
    "#     segment_ids = segment_ids.long().to(device)\n",
    "#     valid_length= valid_length\n",
    "#     label = label.long().to(device)\n",
    "#     out = model(token_ids, valid_length, segment_ids)\n",
    "#     max_vals, max_indices = torch.max(out, 1)\n",
    "#     train_pred.extend(max_indices.cpu().tolist())\n",
    "# \n",
    "# for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(validation_dataloader):\n",
    "#     token_ids = token_ids.long().to(device)\n",
    "#     segment_ids = segment_ids.long().to(device)\n",
    "#     valid_length= valid_length\n",
    "#     label = label.long().to(device)\n",
    "#     out = model(token_ids, valid_length, segment_ids)\n",
    "#     max_vals, max_indices = torch.max(out, 1)\n",
    "#     val_pred.extend(max_indices.cpu().tolist())\n",
    "    \n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    max_vals, max_indices = torch.max(out, 1)\n",
    "    test_pred.extend(max_indices.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b97f1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3382897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = [sentence[1] for sentence in train]\n",
    "val_y = [sentence[1] for sentence in val]\n",
    "test_y = [sentence[1] for sentence in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eeedb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750252780586451\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "test_acc = 0\n",
    "for i in range(len(test_y)):\n",
    "    if test_y[i] == test_pred[i]:\n",
    "        test_acc += 1\n",
    "        \n",
    "print(test_acc/len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d38388d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9927143657762002 0.9931578947368421\n",
      "0.7549751243781095 0.7525125628140703\n",
      "0.7576054955839058 0.7424400417101147\n"
     ]
    }
   ],
   "source": [
    "# fl score\n",
    "\n",
    "print(f1_score(train_y, train_pred, pos_label=0), f1_score(train_y, train_pred, pos_label=1))\n",
    "print(f1_score(val_y, val_pred, pos_label=0), f1_score(val_y, val_pred, pos_label=1))\n",
    "print(f1_score(test_y, test_pred, pos_label=0), f1_score(test_y, test_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02d85d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9921583271097835 0.9936808846761453\n",
      "0.7375455650060754 0.7709137709137709\n",
      "0.7086903304773562 0.8012003000750187\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "\n",
    "print(precision_score(train_y, train_pred, pos_label=0), precision_score(train_y, train_pred, pos_label=1))\n",
    "print(precision_score(val_y, val_pred, pos_label=0), precision_score(val_y, val_pred, pos_label=1))\n",
    "print(precision_score(test_y, test_pred, pos_label=0), precision_score(test_y, test_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07f38319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9932710280373832 0.9926354550236718\n",
      "0.7732484076433122 0.7349693251533742\n",
      "0.8137737174982431 0.6917098445595855\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "\n",
    "print(recall_score(train_y, train_pred, pos_label=0), recall_score(train_y, train_pred, pos_label=1))\n",
    "print(recall_score(val_y, val_pred, pos_label=0), recall_score(val_y, val_pred, pos_label=1))\n",
    "print(recall_score(test_y, test_pred, pos_label=0), recall_score(test_y, test_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bf81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f50efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kcc",
   "language": "python",
   "name": "venv_kcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
